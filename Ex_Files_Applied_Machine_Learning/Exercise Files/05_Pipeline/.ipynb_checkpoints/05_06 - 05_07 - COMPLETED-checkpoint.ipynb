{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will use what we learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data & create train/validation/test set\n",
    "\n",
    "![Eval on Validation](img/evaluate_on_validation.png)\n",
    "\n",
    "****** DON'T FORGET TO COPY RESULTS OVER BELOW **********\n",
    "\n",
    "_Welcome back, now that we've done some hyperparameter tuning we have a good idea of what the best hyperparamter combinations are. I do want to just mention that most of the time you'll test different algorithms as well, in this case we're **just** using `RandomForestClassifier` just to keep things easier._\n",
    "\n",
    "_In this lesson we're going to take those best couple models and really dig into them to see how they're performing on the validation set. You'll notice that we're importing a bunch of additional metrics that we didn't import before. Again, this is just to allow us to dig into the results a little deeper than we were with `cross val score`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "titanic = pd.read_csv('../titanic_cleaned.csv')\n",
    "\n",
    "features = titanic.drop('Survived', axis=1)\n",
    "labels = titanic['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit best models on full training set\n",
    "\n",
    "Results from last section:\n",
    "```\n",
    "0.813 (+/-0.112) for {'max_depth': 2, 'n_estimators': 5}\n",
    "0.8 (+/-0.124) for {'max_depth': 2, 'n_estimators': 50}\n",
    "0.801 (+/-0.117) for {'max_depth': 2, 'n_estimators': 100}\n",
    "0.792 (+/-0.037) for {'max_depth': 10, 'n_estimators': 5}\n",
    "--> 0.82 (+/-0.052) for {'max_depth': 10, 'n_estimators': 50}\n",
    "--> 0.826 (+/-0.048) for {'max_depth': 10, 'n_estimators': 100}\n",
    "0.803 (+/-0.043) for {'max_depth': 20, 'n_estimators': 5}\n",
    "--> 0.822 (+/-0.054) for {'max_depth': 20, 'n_estimators': 50}\n",
    "0.811 (+/-0.051) for {'max_depth': 20, 'n_estimators': 100}\n",
    "0.798 (+/-0.051) for {'max_depth': None, 'n_estimators': 5}\n",
    "0.811 (+/-0.06) for {'max_depth': None, 'n_estimators': 50}\n",
    "0.818 (+/-0.04) for {'max_depth': None, 'n_estimators': 100}\n",
    "```\n",
    "\n",
    "_Here you'll see all of the results we generated using `GridSearchCV` in the last section. We're going to take the three best hyperparameter combinations and we will re-fit those models on the full training set._\n",
    "\n",
    "_So we will start with the first hyperparameter combination with `n estimators` = ?? and `max depth` = ?? and we will store that as `rf1`. Then like we saw before, all we have to do is call `.fit()` on `X train` and `y train`. Now these hyperparameter settings will guide the way this `Random Forest Classifier` fits to the training data._\n",
    "\n",
    "_Now, we want to do this for the top 3 hyperparameter combinations so lets copy this twice, update hyperparameters with the next two options, store as `rf2` and `rf3` and then call `.fit()` on those as well. And it will alter the object in-place, so once this is run `rf1`, `rf2`, and `rf3` become fit models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
    "rf2 = RandomForestClassifier(n_estimators = 100, max_depth = 10)\n",
    "rf3 = RandomForestClassifier(n_estimators = 50, max_depth = 20)\n",
    "\n",
    "rf1.fit(X_train, y_train)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on validation set\n",
    "\n",
    "![Evaluation Metrics](img/eval_metrics.png)\n",
    "\n",
    "_Now, lets evaluate these fit models on the validation set. So the only examples these models have seen are those in the training set. This is the true test, the test of the model's ability to generalize to unseen examples. If they are overfit or underfit they will fail here._\n",
    "\n",
    "_Remember from previous lessons that we will be using accuracy, precision, and recall to evaluate these models to select the one that generalizes to the validation set best._\n",
    "\n",
    "_We're going to use a for loop and we'll just loop through the three fit models we have: `rf1`, `rf2`, and `rf3`. For each model, we will call `.predict()` and that method will ask for you to pass in the features for the validation set. So pass in `X val`. This will generate a list of predictions (survived or not survived) based on the features passed into the model. Now that we have predictions, we need to generate some results metrics:_\n",
    "1. _Lets call accuracy score, and then you need to pass in the **true** values (those are stored in `y val`) and then pass in the predicted values (stored in `y pred`). These two lists are stored in the same order so `accuracy score()` will cycle through the two lists, compare the prediction to the actual outcome for each person and assign an accuracy score for the model based on the validation set_\n",
    "2. _Now lets just copy that down to the next line and just replace accuracy with precision, same example arguments and process - now it just generates precision instead of recall_\n",
    "3. _Copy that down and just replace precision with recall_\n",
    "\n",
    "_Then I created a print statement here that will output the scores for each hyperparameter combination._\n",
    "\n",
    "**TALK THROUGH RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX DEPTH: 10 / # OF EST: 50 -- A: 0.821 / P: 0.844 / R: 0.711\n",
      "MAX DEPTH: 10 / # OF EST: 100 -- A: 0.844 / P: 0.887 / R: 0.724\n",
      "MAX DEPTH: 20 / # OF EST: 50 -- A: 0.804 / P: 0.806 / R: 0.711\n"
     ]
    }
   ],
   "source": [
    "for mdl in [rf1, rf2, rf3]:\n",
    "    y_pred = mdl.predict(X_val)\n",
    "    accuracy = round(accuracy_score(y_val, y_pred), 3)\n",
    "    precision = round(precision_score(y_val, y_pred), 3)\n",
    "    recall = round(recall_score(y_val, y_pred), 3)\n",
    "    print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n",
    "                                                                         mdl.n_estimators,\n",
    "                                                                         accuracy,\n",
    "                                                                         precision,\n",
    "                                                                         recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on the test set\n",
    "\n",
    "![Final Model](img/final_model_selection.png)\n",
    "\n",
    "_Welcome back to the **final** step of this pipeline. Up to this point you've done the following:_\n",
    "1. _Explore the data to learn about feature relationships, shapes, etc._\n",
    "2. _Cleaned up the data to make the signal clear and easy for a model to pick up_\n",
    "3. _Split the data into train/validation/test and fit a basic model using Cross-Validation on the training set to set a baseline_\n",
    "4. _Explore 12 different hyperparameter combinations using `GridSearchCV` to figure out which hyperparameters settings would generate the best model_\n",
    "5. _Refit the best few models on the full training set and evaluate it on the validation set to select the **best** model_\n",
    "\n",
    "_And it all culminates with this final step. Now that we have the best model, we need to evaluate it on the test set to get a truly unbiased view of how it should perform moving forward. Now you might want to ask: \"why do we need to evaluate it on the test set when we already evaluated this model on unseen data with the validation set\"?_\n",
    "\n",
    "_That's a great question, the validation set could still be a somewhat biased view in the sense that you selected the best model BASED on the performance on that validation set. If the validation set was slightly different, perhaps you would choose a different model. By nature of the fact that it impacted what model we chose as our final model, it's part of the model training process. This final test set is purely for evaluation purposes to see if it matches performance that we've seen before._\n",
    "\n",
    "_Now, we're basically just replicating all of the code we saw in the last section. We're using our best model that was already fit on the full training set, we're making predictions based on `X test`, we're generating accuracy, precision, and recall, and printing it all out._\n",
    "\n",
    "**GO INTO DETAIL ABOUT WHAT THE PERFORMANCE WAS USING `GRIDSEARCHCV` COMPARED TO PERFORMANCE ON VALIDATION SET TO PERFORMANCE ON TEST SET AND DRAW SOME CONCLUSIONS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX DEPTH: 20 / # OF EST: 50 -- A: 0.787 / P: 0.737 / R: 0.646\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf2.predict(X_test)\n",
    "accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "precision = round(precision_score(y_test, y_pred), 3)\n",
    "recall = round(recall_score(y_test, y_pred), 3)\n",
    "print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n",
    "                                                                     mdl.n_estimators,\n",
    "                                                                     accuracy,\n",
    "                                                                     precision,\n",
    "                                                                     recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
