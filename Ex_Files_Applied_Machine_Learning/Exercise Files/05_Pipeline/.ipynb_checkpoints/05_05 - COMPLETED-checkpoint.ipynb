{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Tune hyperparameters\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will tune the hyperparameters for the basic model we fit in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data & create train/validation/test set\n",
    "\n",
    "![Tune Hyperparameters](img/tune_hyperparameters.png)\n",
    "\n",
    "_Welcome back to lesson five, we're going to build off our last lesson and we will still use Cross-Validation on the training set but we're going to add one more layer. We're going to run GridSearch to find the optimal hyperparameter settings for our model._\n",
    "\n",
    "_In addition to `RandomForestClassifier` and `train test split` - you'll notice that the `cross val score` we used before is now replaced by `GridSearchCV`. All `GridSearchCV` is is a wrapper around `cross val score` that allows us to run Grid-Search within `Cross-Validation`._\n",
    "\n",
    "_So now we'll import our data and creating our training, test, and validation sets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "titanic = pd.read_csv('../titanic_cleaned.csv')\n",
    "\n",
    "features = titanic.drop('Survived', axis=1)\n",
    "labels = titanic['Survived']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "![Hyperparameters](img/hyperparameters.png)\n",
    "\n",
    "_I wrote a quick little function here for us to use to print the results. I'm not going to go through it in detail but in essence what it does is for every hyper-parameter combination it will print out the average accuracy score (again, remember there will be 5 accuracy scores - one for each fold) and the standard deviation of that accuracy score. This will give us the information we need to select the optimal hyperparameter settings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now, we're using `RandomForestClassifier` again. This course isn't really meant to get too in-depth into the individual algorithms but essentially what a `Random Forest` is is a collection of decision trees. So that decision tree above, a `Random Forest` would be if we built 5 or 10 or 100 of those and then they all worked together to determine the ultimate prediction. Each individual decision tree would be fit on some subset of data and some subset of features. It's not critical that you understand this too in depth at this point._\n",
    "\n",
    "_With that background - we need to define our parameter search region. There are two `parameters` we want to tune:_\n",
    "1. _Number of estimators, this means **how many** individual decision trees do we want to build within our `Random Forest`_\n",
    "2. _Max Depth - we mentioned this in the slide above, this will just dictate how deep each of the individual decision trees go_\n",
    "\n",
    "_So lets say we want to test using 5, 50, and 100 individual decision trees and we want to test max depth of 2, 10, 20, and None._\n",
    "\n",
    "_Then we will just call the `GridSearchCV` method, we pass in our model, then we pass in our parameter diction, and lastly we just tell it that we want 5 folds. Then we store that as `cv`. If you've ever used `scikit-learn` before you'll notice that their model training API is exactly the same for any type of model. You store the object and then you call `.fit()`. So we will do that with `cv` and we pass in `X train` and `y train`._\n",
    "\n",
    "_Lastly, just call the function I wrote about: `print results(cv)`_\n",
    "\n",
    "_Now, what this is doing under the hood is it's taking each parameter combination (3 levels of estimators, 4 levels of max depth so 12 total combinations) - for each combination it's running 5-fold Cross-Validation. So essentially it's building 60 models under the hood - one model for each fold, 5 folds for each combination, 12 total combinations._\n",
    "\n",
    "_Ok, now lets take a look at the results. I just want to highlight again, these hyperparameters are facilitating how the Random Forest Classifier fits to the data so it will determine that bias variance trade-off and whether this is overfitting or underfitting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS: {'max_depth': 10, 'n_estimators': 100}\n",
      "\n",
      "0.747 (+/-0.124) for {'max_depth': 2, 'n_estimators': 5}\n",
      "0.794 (+/-0.123) for {'max_depth': 2, 'n_estimators': 50}\n",
      "0.8 (+/-0.122) for {'max_depth': 2, 'n_estimators': 100}\n",
      "0.794 (+/-0.049) for {'max_depth': 10, 'n_estimators': 5}\n",
      "0.82 (+/-0.039) for {'max_depth': 10, 'n_estimators': 50}\n",
      "0.831 (+/-0.064) for {'max_depth': 10, 'n_estimators': 100}\n",
      "0.809 (+/-0.073) for {'max_depth': 20, 'n_estimators': 5}\n",
      "0.805 (+/-0.034) for {'max_depth': 20, 'n_estimators': 50}\n",
      "0.811 (+/-0.047) for {'max_depth': 20, 'n_estimators': 100}\n",
      "0.803 (+/-0.033) for {'max_depth': None, 'n_estimators': 5}\n",
      "0.813 (+/-0.052) for {'max_depth': None, 'n_estimators': 50}\n",
      "0.816 (+/-0.024) for {'max_depth': None, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djedamski/.pyenv/versions/3.6.2/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100],\n",
    "    'max_depth': [2, 10, 20, None]\n",
    "}\n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
